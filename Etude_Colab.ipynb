{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "# Etude - Piano Cover Generation\n",
        "\n",
        "This notebook allows you to run the Etude pipeline directly from Google Drive.\n",
        "\n",
        "## Setup\n",
        "\n",
        "1.  **Mount Google Drive**: This allows the notebook to access your files.\n",
        "2.  **Install Dependencies**: Installs the necessary libraries (Demucs, PyTorch, Madmom, etc.).\n",
        "3.  **Download Models**: Downloads the pre-trained checkpoints required for inference.\n",
        "4.  **Navigate to Project**: Changes the working directory to where you cloned/uploaded the Etude repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount-drive"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "change-dir"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Change this path to match where you uploaded/cloned the Etude folder in your Drive.\n",
        "# Example: '/content/drive/MyDrive/Projects/Etude'\n",
        "PROJECT_PATH = '/content/drive/MyDrive/Etude'\n",
        "# ---------------------\n",
        "\n",
        "if not os.path.exists(PROJECT_PATH):\n",
        "    print(f\"WARNING: Path {PROJECT_PATH} does not exist. Please check your Drive structure.\")\n",
        "else:\n",
        "    os.chdir(PROJECT_PATH)\n",
        "    print(f\"Current working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-deps-header"
      },
      "source": [
        "## Install Dependencies\n",
        "We need to install system libraries (ffmpeg) and Python packages.\n",
        "\n",
        "**Note on Dependencies:**\n",
        "The `synctoolbox` library requires `numpy<2.0` and `pandas<2.0`. We explicitly install these older versions to ensure compatibility. This might cause pip to show warnings about `google-colab` or other pre-installed packages being incompatible, but this is expected and necessary for the audio processing pipeline to work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-system-deps"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update && sudo apt-get install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-build-deps"
      },
      "outputs": [],
      "source": [
        "# Ensure Cython is available for building madmom, but don't force a version.\n",
        "!pip install Cython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-reqs"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "patch-madmom"
      },
      "outputs": [],
      "source": [
        "# Install Madmom from source with patching\n",
        "# We clone the repo, patch the source code (fixing np.int/np.float issues), and then install.\n",
        "# This ensures the Cython extensions are compiled with the correct NumPy types.\n",
        "\n",
        "import os\n",
        "if not os.path.exists('madmom'):\n",
        "    # IMPORTANT: Use --recursive to fetch the 'models' submodule!\n",
        "    !git clone --recursive https://github.com/CPJKU/madmom.git\n",
        "\n",
        "# Run the patch script on the SOURCE directory\n",
        "!python fix_madmom.py --path madmom\n",
        "\n",
        "# Install from the patched source\n",
        "!pip install ./madmom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check-versions"
      },
      "outputs": [],
      "source": [
        "# Verify Installation & Versions\n",
        "import torch\n",
        "import torchvision\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import madmom\n",
        "\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(f\"Torchvision version: {torchvision.__version__}\")\n",
        "print(f\"Torchaudio version: {torchaudio.__version__}\")\n",
        "print(f\"Madmom version: {madmom.__version__}\")\n",
        "\n",
        "print(\"\\n--- Verifying Madmom Fixes ---\")\n",
        "try:\n",
        "    # Test 1: Imports (Fixes MutableSequence)\n",
        "    from madmom.features.beats import DBNBeatTrackingProcessor\n",
        "    from madmom.features.downbeats import DBNDownBeatTrackingProcessor\n",
        "    print(\"✅ Imports successful!\")\n",
        "\n",
        "    # Test 2: Initialization (Fixes np.int in compiled code)\n",
        "    beat_processor = DBNBeatTrackingProcessor(fps=100)\n",
        "    downbeat_processor = DBNDownBeatTrackingProcessor(beats_per_bar=4, fps=100)\n",
        "    print(\"✅ Processors initialized successfully!\")\n",
        "\n",
        "    # Test 3: Dummy Inference\n",
        "    dummy_beat_activation = np.random.rand(1000)\n",
        "    beats = beat_processor(dummy_beat_activation)\n",
        "    print(f\"✅ Inference successful! Detected {len(beats)} dummy beats.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Verification failed: {e}\")\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download-models-header"
      },
      "source": [
        "## Download Pre-trained Models\n",
        "This step downloads the necessary model checkpoints and extracts them to the `checkpoints/` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-models"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('checkpoints'):\n",
        "    print(\"Downloading checkpoints...\")\n",
        "    !wget -O checkpoints.zip \"https://github.com/Xiugapurin/Etude/releases/download/latest/checkpoints.zip\"\n",
        "    !unzip -q checkpoints.zip\n",
        "    !rm checkpoints.zip\n",
        "    print(\"Checkpoints downloaded and extracted.\")\n",
        "else:\n",
        "    print(\"Checkpoints directory already exists. Skipping download.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pipeline-header"
      },
      "source": [
        "## Run Pipeline\n",
        "\n",
        "You can now run the data preparation or inference scripts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inference-header"
      },
      "source": [
        "### Inference (Generate Piano Cover)\n",
        "Use `infer.py` to generate a cover from a YouTube URL or local file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-inference"
      },
      "outputs": [],
      "source": [
        "# Example: Generate a cover from a YouTube URL\n",
        "# Replace the URL with your desired song.\n",
        "!python infer.py --input \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\" --output_name \"my_cover\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "resume-header"
      },
      "source": [
        "### Resuming from a Crash\n",
        "If the pipeline crashes (e.g., during Stage 2), you can resume from a specific stage using `--start-from`.\n",
        "\n",
        "- `extract`: Start from beginning (Stage 1)\n",
        "- `structuralize`: Start from Stage 2 (skips extraction)\n",
        "- `decode`: Start from Stage 3 (skips extraction and beat detection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-resume"
      },
      "outputs": [],
      "source": [
        "# Example: Resume from Stage 2 (Structuralize)\n",
        "# !python infer.py --input \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\" --output_name \"my_cover\" --start-from structuralize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization-header"
      },
      "source": [
        "## Visualization and Playback\n",
        "Run these cells to listen to your generated MIDI and view it as sheet music."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-viz-deps"
      },
      "outputs": [],
      "source": [
        "# Install dependencies for playback (FluidSynth) and visualization (LilyPond)\n",
        "!sudo apt-get install -y fluidsynth fluid-soundfont-gm lilypond\n",
        "!pip install pyfluidsynth music21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "play-midi"
      },
      "outputs": [],
      "source": [
        "import pretty_midi\n",
        "from IPython.display import Audio, display\n",
        "import numpy as np\n",
        "\n",
        "def play_midi(midi_path):\n",
        "    print(f\"Synthesizing {midi_path}...\")\n",
        "    pm = pretty_midi.PrettyMIDI(str(midi_path))\n",
        "    # Synthesize audio using the installed SoundFont\n",
        "    # Sampling rate 44100Hz\n",
        "    audio_data = pm.fluidsynth(fs=44100)\n",
        "    display(Audio(audio_data, rate=44100))\n",
        "\n",
        "# Replace with your actual output filename\n",
        "output_midi = \"outputs/inference/my_cover.mid\"\n",
        "if os.path.exists(output_midi):\n",
        "    play_midi(output_midi)\n",
        "else:\n",
        "    print(f\"File not found: {output_midi}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "show-sheet-music"
      },
      "outputs": [],
      "source": [
        "from music21 import converter, environment\n",
        "from IPython.display import Image, display\n",
        "import glob\n",
        "\n",
        "# Configure music21 to use LilyPond\n",
        "us = environment.UserSettings()\n",
        "us['lilypondPath'] = '/usr/bin/lilypond'\n",
        "\n",
        "def show_sheet_music(midi_path, start_measure=None, end_measure=None):\n",
        "    print(f\"Rendering sheet music for {midi_path}...\")\n",
        "    try:\n",
        "        s = converter.parse(str(midi_path))\n",
        "        \n",
        "        # Filter measures if requested\n",
        "        if start_measure is not None and end_measure is not None:\n",
        "            s_to_render = s.measures(start_measure, end_measure)\n",
        "            print(f\"Rendering measures {start_measure} to {end_measure}...\")\n",
        "        else:\n",
        "            s_to_render = s\n",
        "            print(\"Rendering full score (this may take a moment)...\")\n",
        "        \n",
        "        # Render to image(s)\n",
        "        # music21 with lilypond backend generates 'lily.png' or 'lily-page1.png', 'lily-page2.png' etc.\n",
        "        # We use a base filename and check for outputs.\n",
        "        base_name = 'sheet_music'\n",
        "        image_path = s_to_render.write('lily.png', fp=base_name)\n",
        "        \n",
        "        # Check if multiple pages were generated\n",
        "        # music21 returns the path to the first file usually\n",
        "        \n",
        "        # Display logic\n",
        "        generated_files = sorted(glob.glob(f\"{base_name}*.png\"))\n",
        "        \n",
        "        if not generated_files:\n",
        "             # Fallback if the return path is specific and glob didn't catch it (unlikely with lily.png)\n",
        "             if os.path.exists(str(image_path)):\n",
        "                 generated_files = [str(image_path)]\n",
        "        \n",
        "        if generated_files:\n",
        "            for img_file in generated_files:\n",
        "                print(f\"Displaying {img_file}...\")\n",
        "                display(Image(filename=img_file))\n",
        "        else:\n",
        "            print(\"No image files were generated.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error rendering sheet music: {e}\")\n",
        "\n",
        "# Replace with your actual output filename\n",
        "output_midi = \"outputs/inference/my_cover.mid\"\n",
        "if os.path.exists(output_midi):\n",
        "    # Default to full score, but you can pass start_measure=1, end_measure=10 to limit it\n",
        "    show_sheet_music(output_midi)\n",
        "else:\n",
        "    print(f\"File not found: {output_midi}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training-header"
      },
      "source": [
        "### Data Preparation (Optional)\n",
        "If you are training the model, use `prepare.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-prepare"
      },
      "outputs": [],
      "source": [
        "!python prepare.py --start-from download"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}