# infer.py

import sys
import yaml
import shutil
import argparse
import subprocess
from pathlib import Path
from urllib.parse import urlparse
import torch

from etude.data.extractor import AMTAPC_Extractor
from etude.data.beat_analyzer import BeatAnalyzer
from etude.data.tokenizer import TinyREMITokenizer
from etude.data.vocab import Vocab
from etude.utils.preprocess import analyze_volume, save_volume_map
from etude.utils.model_loader import load_etude_decoder
from etude.utils.download import download_audio_from_url


class InferencePipeline:
    """Orchestrates the entire inference process from audio to final MIDI."""
    def __init__(self, config: dict):
        self.config = config
        with open("configs/project_config.yaml", 'r') as f:
            self.project_config = yaml.safe_load(f)

        self.device = config['general']['device']
        if self.device == 'auto':
            self.device = "cuda" if torch.cuda.is_available() else "cpu"

        self.output_dir = Path(config['general']['output_dir'])
        self.work_dir = self.output_dir / "temp"
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.work_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"[INFO] Device: {self.device}")
        print(f"[INFO] Final outputs will be saved to: {self.output_dir.resolve()}")
        print(f"[INFO] Intermediate files are stored in: {self.work_dir.resolve()}")

    def _run_command(self, command: list):
        """Helper to run a command and check for errors."""
        try:
            result = subprocess.run(command, check=True, capture_output=True, text=True, encoding='utf-8')
        except subprocess.CalledProcessError as e:
            print(f"[ERROR] Error executing command: {' '.join(command)}", file=sys.stderr)
            print(f"  {e.stderr}", file=sys.stderr)
            sys.exit(1)

    def _prepare_audio(self, source: str) -> Path:
        """Ensures the source audio is available locally in the working directory."""
        print("    > Preparing source audio.")
        local_audio_path = self.work_dir / "origin.wav"

        if urlparse(source).scheme in ('http', 'https'):
            success = download_audio_from_url(source, local_audio_path)
            if not success:
                print("[ERROR] Audio download failed. Exiting.", file=sys.stderr)
                sys.exit(1)
        elif Path(source).is_file():
            shutil.copy(source, local_audio_path)
        else:
            print(f"[ERROR] Input source '{source}' is not a valid URL or local file.", file=sys.stderr)
            sys.exit(1)
        
        return local_audio_path

    def _run_stage1_extract(self, audio_path: Path):
        """Runs the AMT extraction process."""
        print("\n[STAGE 1] Extracting feature notes...")
        extract_config = self.config['extract']
        with open(extract_config['config_path'], 'r') as f: 
            amtapc_config = yaml.safe_load(f)
        
        extractor = AMTAPC_Extractor(config=amtapc_config, model_path=extract_config['model_path'], device=self.device)
        extractor.extract(
            audio_path=str(audio_path),
            output_json_path=str(self.work_dir / "extract.json")
        )

        print("    > Analyzing volume contour.")
        volume_map = analyze_volume(audio_path)
        save_volume_map(volume_map, self.work_dir / "volume.json")

    def _run_stage2_structuralize(self, audio_path: Path):
        """Runs the beat detection and tempo analysis process."""
        print("\n[STAGE 2] Structuralizing tempo information.")
        spleeter_cmd = [ "python", "scripts/run_separation.py",
                         "--input", str(audio_path), "--output", str(self.work_dir / "sep.npy") ]
        print("    > Running source separation for beat detection...")
        self._run_command(spleeter_cmd)

        beat_detection_cmd = [ "python", "scripts/run_beat_detection.py",
                               "--input_npy", str(self.work_dir / "sep.npy"),
                               "--output_json", str(self.work_dir / "beat_pred.json"),
                               "--model_path", self.config['structuralize']['beat_model_path'],
                               "--config_path", self.config['structuralize']['config_path'] ]
        print("    > Running beat detection...")
        self._run_command(beat_detection_cmd)

        print("    > Analyzing beats to generate tempo structure...")
        beat_analyzer = BeatAnalyzer()
        tempo_data = beat_analyzer.analyze(self.work_dir / "beat_pred.json")
        beat_analyzer.save_tempo_data(tempo_data, self.work_dir / "tempo.json")

    def _run_stage3_decode(self, target_attributes: dict, final_filename: str):
        """Runs the final music generation based on the intermediate files."""
        print("\n[STAGE 3] Decoding with target attributes to generate piano cover.")
        decode_config = self.config['decoder']
        model = load_etude_decoder(decode_config['config_path'], decode_config['model_path'], self.device)
        vocab = Vocab.load(decode_config['vocab_path'])
        tokenizer = TinyREMITokenizer(tempo_path=self.work_dir / "tempo.json")

        condition_events = tokenizer.encode(str(self.work_dir / "extract.json"))
        condition_ids = vocab.encode_sequence(condition_events)

        all_x_bars = tokenizer.split_sequence_into_bars(condition_ids, vocab.get_bar_bos_id(), vocab.get_bar_eos_id())
        num_bars = len(all_x_bars)
        target_attributes_per_bar = [target_attributes] * num_bars
        
        print(f"    > Generating {num_bars} bars of music.")
        generated_events = model.generate(
            vocab=vocab, 
            all_x_bars=all_x_bars,
            target_attributes_per_bar=target_attributes_per_bar,
            temperature=decode_config['temperature'], 
            top_p=decode_config['top_p']
        )
        
        if generated_events:
            print("    > Decoding generated events into final MIDI.")
            final_notes = tokenizer.decode_to_notes(
                events=generated_events, 
                volume_map_path=self.work_dir / "volume.json"
            )
            final_midi_path = self.output_dir / f"{final_filename}.mid"
            tokenizer.note_to_midi(final_notes, final_midi_path)
            print(f"[INFO] Final MIDI saved to: {final_midi_path.resolve()}")
        else:
            print("[WARN] Model generated an empty sequence.")

    def run(self, audio_source: str, target_attributes: dict, final_filename: str, start_from: str = 'extract'):
        """Executes the inference pipeline, conditionally skipping stages."""
        
        # Determine which stages to run based on start_from
        # Stages: extract -> structuralize -> decode
        stages = ['extract', 'structuralize', 'decode']
        if start_from not in stages:
             print(f"[ERROR] Invalid start_from stage: {start_from}. Must be one of {stages}", file=sys.stderr)
             sys.exit(1)
             
        start_idx = stages.index(start_from)
        run_extract = (start_idx <= 0)
        run_struct = (start_idx <= 1)
        run_decode = (start_idx <= 2) # Always true effectively

        # Prepare audio is needed if we run any processing stage, or if we need it for checking
        # But if we skip extract, we assume intermediate files exist.
        # However, stage 2 needs the audio file for separation.
        
        local_audio_path = self.work_dir / "origin.wav"
        
        # If running from scratch or structuralize, we need the audio
        if run_extract or run_struct:
            if not local_audio_path.exists() or run_extract:
                 # If we are starting from extract, we always prepare. 
                 # If starting from structuralize, we need audio. If it's missing, we must prepare it.
                 self._prepare_audio(audio_source)
            else:
                 print(f"[INFO] Found existing audio at {local_audio_path}, skipping download/copy.")

        # --- Stage 1: Extract ---
        if run_extract:
            self._run_stage1_extract(local_audio_path)
        else:
            print("[INFO] Skipping Stage 1 (Extract). Checking for required files...")
            if not (self.work_dir / "extract.json").exists() or not (self.work_dir / "volume.json").exists():
                 print("[ERROR] Missing 'extract.json' or 'volume.json'. Cannot skip Stage 1.", file=sys.stderr)
                 sys.exit(1)

        # --- Stage 2: Structuralize ---
        if run_struct:
            self._run_stage2_structuralize(local_audio_path)
        else:
            print("[INFO] Skipping Stage 2 (Structuralize). Checking for required files...")
            if not (self.work_dir / "tempo.json").exists():
                 print("[ERROR] Missing 'tempo.json'. Cannot skip Stage 2.", file=sys.stderr)
                 sys.exit(1)

        # --- Stage 3: Decode ---
        if run_decode:
            self._run_stage3_decode(target_attributes, final_filename)

        print("\n[INFO] Inference pipeline finished successfully!")


def main():
    parser = argparse.ArgumentParser(description="End-to-end piano cover generation pipeline.")
    parser.add_argument("--config", type=str, default="configs/inference_config.yaml", help="Path to the main inference config file.")
    parser.add_argument("--output_name", type=str, default="output", help="Base name for the final output MIDI file (without extension).")

    source_group = parser.add_mutually_exclusive_group(required=True)
    source_group.add_argument("--input", type=str, help="Path or URL to the source audio file for a full run.")
    source_group.add_argument("--decode-only", action="store_true", help="[Deprecated] Use --start-from decode instead.")
    
    parser.add_argument("--start-from", type=str, default="extract", choices=["extract", "structuralize", "decode"],
                        help="Start the pipeline from a specific stage. Useful for resuming crashed runs.")
    
    attr_group = parser.add_argument_group('Target Attribute Controls')
    attr_group.add_argument("--polyphony", type=int, default=1, choices=[0, 1, 2], help="Target bin for Relative Polyphony.")
    attr_group.add_argument("--rhythm", type=int, default=1, choices=[0, 1, 2], help="Target bin for Relative Rhythmic Intensity.")
    attr_group.add_argument("--sustain", type=int, default=1, choices=[0, 1, 2], help="Target bin for Relative Note Sustain.")
    attr_group.add_argument(
        "--overlap", 
        type=int, 
        default=2,
        choices=[0, 1, 2], 
        help="Target bin for Pitch Overlap Ratio. [WARNING: For best quality, this should be kept at 2.]"
    )
    args = parser.parse_args()

    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
        
    target_attributes = {
        "polyphony_bin": args.polyphony, 
        "rhythm_intensity_bin": args.rhythm,
        "sustain_bin": args.sustain, 
        "pitch_overlap_bin": args.overlap
    }
    
    # Handle legacy decode-only flag
    start_from = args.start_from
    if args.decode_only:
        start_from = 'decode'
        
    pipeline = InferencePipeline(config)
    pipeline.run(
        audio_source=args.input, 
        target_attributes=target_attributes, 
        final_filename=args.output_name,
        start_from=start_from
    )

if __name__ == "__main__":
    main()